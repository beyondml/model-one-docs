{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tune_the_model\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_the_model.set_api_key('eyJhbGci...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tweet_eval\", \"irony\")\n",
    "train = pd.DataFrame(dataset['train'])\n",
    "validation = pd.DataFrame(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tune_the_model.train_classifier(r'tune-the-model-tweet_eval-irony.json',\n",
    "                    train['text'], train['label'], validation['text'], validation['label'])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wait_for_training_finish()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_validation = []\n",
    "for text in dataset['validation']['text']:\n",
    "    res_validation += model.classify(input=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(dataset['validation']['label'], res_validation)\n",
    "f1_scores = 2*recall*precision/(recall+precision)\n",
    "\n",
    "threshold = thresholds[np.argmax(f1_scores)]\n",
    "print('Best threshold: ', threshold)\n",
    "print('Best F1-Score: ', np.max(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for text in dataset['test']['text']:\n",
    "    res += model.classify(input=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.61      0.72       473\n",
      "           1       0.60      0.86      0.71       311\n",
      "\n",
      "    accuracy                           0.71       784\n",
      "   macro avg       0.73      0.74      0.71       784\n",
      "weighted avg       0.76      0.71      0.71       784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = 1*(np.array(res) > threshold)\n",
    "\n",
    "print(classification_report(dataset['test']['label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50590d8a5853efeb72b3c83e8ac776925d5725c2922993cff66a538573ac4ff6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
